// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: sogou/speech/asr/v1/asr.proto

// This CPP symbol can be defined to use imports that match up to the framework
// imports needed when using CocoaPods.
#if !defined(GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS)
 #define GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS 0
#endif

#if GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS
 #import <Protobuf/GPBProtocolBuffers.h>
#else
 #import "GPBProtocolBuffers.h"
#endif

#if GOOGLE_PROTOBUF_OBJC_VERSION < 30002
#error This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer library sources.
#endif
#if 30002 < GOOGLE_PROTOBUF_OBJC_MIN_SUPPORTED_VERSION
#error This file was generated by an older version of protoc which is incompatible with your Protocol Buffer library sources.
#endif

// @@protoc_insertion_point(imports)

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-declarations"

CF_EXTERN_C_BEGIN

@class GPBAny;
@class GPBDuration;
@class GPBTimestamp;
@class RPCStatus;
@class SPBRecognitionAudio;
@class SPBRecognitionConfig;
@class SPBSpeechContext;
@class SPBSpeechRecognitionAlternative;
@class SPBSpeechRecognitionResult;
@class SPBStreamingRecognitionConfig;
@class SPBStreamingRecognitionResult;
@class SPBWordInfo;

NS_ASSUME_NONNULL_BEGIN

#pragma mark - Enum SPBRecognitionConfig_AudioEncoding

/**
 * Audio encoding of the data sent in the audio message. All encodings support
 * only 1 channel (mono) audio. Only `FLAC` include a header that
 * describes the bytes of audio that follow the header. The other encodings
 * are raw audio bytes with no header.
 *
 * For best results, the audio source should be captured and transmitted using
 * a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
 * reduced if lossy codecs, which include the other codecs listed in
 * this section, are used to capture or transmit the audio, particularly if
 * background noise is present.
 **/
typedef GPB_ENUM(SPBRecognitionConfig_AudioEncoding) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  SPBRecognitionConfig_AudioEncoding_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** Not specified. Will return result status 400. */
  SPBRecognitionConfig_AudioEncoding_EncodingUnspecified = 0,

  /** Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  SPBRecognitionConfig_AudioEncoding_Linear16 = 1,

  /**
   * [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
   * Codec) is the recommended encoding because it is
   * lossless--therefore recognition is not compromised--and
   * requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
   * encoding only supports 16-bit samples, however, not all fields in
   * `STREAMINFO` are supported.
   **/
  SPBRecognitionConfig_AudioEncoding_Flac = 2,

  /** Sogou specific speex format 16-bit samples 1 channel */
  SPBRecognitionConfig_AudioEncoding_SogouSpeex = 100,
};

GPBEnumDescriptor *SPBRecognitionConfig_AudioEncoding_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL SPBRecognitionConfig_AudioEncoding_IsValidValue(int32_t value);

#pragma mark - Enum SPBStreamingRecognizeResponse_SpeechEventType

/** Indicates the type of speech event. */
typedef GPB_ENUM(SPBStreamingRecognizeResponse_SpeechEventType) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  SPBStreamingRecognizeResponse_SpeechEventType_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** No speech event specified. */
  SPBStreamingRecognizeResponse_SpeechEventType_SpeechEventUnspecified = 0,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This event is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   **/
  SPBStreamingRecognizeResponse_SpeechEventType_EndOfSingleUtterance = 1,
};

GPBEnumDescriptor *SPBStreamingRecognizeResponse_SpeechEventType_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL SPBStreamingRecognizeResponse_SpeechEventType_IsValidValue(int32_t value);

#pragma mark - SPBAsrRoot

/**
 * Exposes the extension registry for this file.
 *
 * The base class provides:
 * @code
 *   + (GPBExtensionRegistry *)extensionRegistry;
 * @endcode
 * which is a @c GPBExtensionRegistry that includes all the extensions defined by
 * this file and all files that it depends on.
 **/
@interface SPBAsrRoot : GPBRootObject
@end

#pragma mark - SPBRecognizeRequest

typedef GPB_ENUM(SPBRecognizeRequest_FieldNumber) {
  SPBRecognizeRequest_FieldNumber_Config = 1,
  SPBRecognizeRequest_FieldNumber_Audio = 2,
};

/**
 * The top-level message sent by the client for the `Recognize` method.
 **/
@interface SPBRecognizeRequest : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) SPBRecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/** *Required* The audio data to be recognized. */
@property(nonatomic, readwrite, strong, null_resettable) SPBRecognitionAudio *audio;
/** Test to see if @c audio has been set. */
@property(nonatomic, readwrite) BOOL hasAudio;

@end

#pragma mark - SPBLongRunningRecognizeRequest

typedef GPB_ENUM(SPBLongRunningRecognizeRequest_FieldNumber) {
  SPBLongRunningRecognizeRequest_FieldNumber_Config = 1,
  SPBLongRunningRecognizeRequest_FieldNumber_Audio = 2,
};

/**
 * The top-level message sent by the client for the `LongRunningRecognize`
 * method.
 **/
@interface SPBLongRunningRecognizeRequest : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) SPBRecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/** *Required* The audio data to be recognized. */
@property(nonatomic, readwrite, strong, null_resettable) SPBRecognitionAudio *audio;
/** Test to see if @c audio has been set. */
@property(nonatomic, readwrite) BOOL hasAudio;

@end

#pragma mark - SPBStreamingRecognizeRequest

typedef GPB_ENUM(SPBStreamingRecognizeRequest_FieldNumber) {
  SPBStreamingRecognizeRequest_FieldNumber_StreamingConfig = 1,
  SPBStreamingRecognizeRequest_FieldNumber_AudioContent = 2,
};

typedef GPB_ENUM(SPBStreamingRecognizeRequest_StreamingRequest_OneOfCase) {
  SPBStreamingRecognizeRequest_StreamingRequest_OneOfCase_GPBUnsetOneOfCase = 0,
  SPBStreamingRecognizeRequest_StreamingRequest_OneOfCase_StreamingConfig = 1,
  SPBStreamingRecognizeRequest_StreamingRequest_OneOfCase_AudioContent = 2,
};

/**
 * The top-level message sent by the client for the `StreamingRecognize` method.
 * Multiple `StreamingRecognizeRequest` messages are sent. The first message
 * must contain a `streaming_config` message and must not contain `audio` data.
 * All subsequent messages must contain `audio` data and must not contain a
 * `streaming_config` message.
 **/
@interface SPBStreamingRecognizeRequest : GPBMessage

/** The streaming request, which is either a streaming config or audio content. */
@property(nonatomic, readonly) SPBStreamingRecognizeRequest_StreamingRequest_OneOfCase streamingRequestOneOfCase;

/**
 * Provides information to the recognizer that specifies how to process the
 * request. The first `StreamingRecognizeRequest` message must contain a
 * `streaming_config`  message.
 **/
@property(nonatomic, readwrite, strong, null_resettable) SPBStreamingRecognitionConfig *streamingConfig;

/**
 * The audio data to be recognized. Sequential chunks of audio data are sent
 * in sequential `StreamingRecognizeRequest` messages. The first
 * `StreamingRecognizeRequest` message must not contain `audio_content` data
 * and all subsequent `StreamingRecognizeRequest` messages must contain
 * `audio_content` data. The audio bytes must be encoded as specified in
 * `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 * pure binary representation (not base64).
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSData *audioContent;

@end

/**
 * Clears whatever value was set for the oneof 'streamingRequest'.
 **/
void SPBStreamingRecognizeRequest_ClearStreamingRequestOneOfCase(SPBStreamingRecognizeRequest *message);

#pragma mark - SPBStreamingRecognitionConfig

typedef GPB_ENUM(SPBStreamingRecognitionConfig_FieldNumber) {
  SPBStreamingRecognitionConfig_FieldNumber_Config = 1,
  SPBStreamingRecognitionConfig_FieldNumber_SingleUtterance = 2,
  SPBStreamingRecognitionConfig_FieldNumber_InterimResults = 3,
  SPBStreamingRecognitionConfig_FieldNumber_EnableInterimResultsPunctuation = 4,
  SPBStreamingRecognitionConfig_FieldNumber_MergeUtterance = 5,
};

/**
 * Provides information to the recognizer that specifies how to process the
 * request.
 **/
@interface SPBStreamingRecognitionConfig : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) SPBRecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/**
 * *Optional* If `false` or omitted, the recognizer will perform continuous
 * recognition (continuing to wait for and process audio even if the user
 * pauses speaking) until the client closes the input stream (gRPC API) or
 * until the maximum time limit has been reached. May return multiple
 * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
 *
 * If `true`, the recognizer will detect a single spoken utterance. When it
 * detects that the user has paused or stopped speaking, it will return an
 * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
 * more than one `StreamingRecognitionResult` with the `is_final` flag set to
 * `true`.
 **/
@property(nonatomic, readwrite) BOOL singleUtterance;

/**
 * *Optional* If `true`, interim results (tentative hypotheses) may be
 * returned as they become available (these interim results are indicated with
 * the `is_final=false` flag).
 * If `false` or omitted, only `is_final=true` result(s) are returned.
 **/
@property(nonatomic, readwrite) BOOL interimResults;

/**
 * *Optional* If `true`, adds punctuation to interim recognition result hypotheses.
 * available only if RecognitionConfig.disable_automatic_punctuation = false AND interim_results = true
 *
 * NOTE: premium features.
 **/
@property(nonatomic, readwrite) BOOL enableInterimResultsPunctuation;

/**
 * *Optional* similar to `single_utterance`. But if `true`,
 * the recognizer will NOT detect any single spoken utterance
 * until the client closes the input stream (gRPC API) or
 * until the maximum time limit has been reached (approximately 30 seconds).
 * It will return only one `StreamingRecognitionResult`s with the `is_final` set to `true`
 * with an `END_OF_SINGLE_UTTERANCE` event and cease recognition.
 **/
@property(nonatomic, readwrite) BOOL mergeUtterance;

@end

#pragma mark - SPBRecognitionConfig

typedef GPB_ENUM(SPBRecognitionConfig_FieldNumber) {
  SPBRecognitionConfig_FieldNumber_Encoding = 1,
  SPBRecognitionConfig_FieldNumber_SampleRateHertz = 2,
  SPBRecognitionConfig_FieldNumber_LanguageCode = 3,
  SPBRecognitionConfig_FieldNumber_MaxAlternatives = 4,
  SPBRecognitionConfig_FieldNumber_ProfanityFilter = 5,
  SPBRecognitionConfig_FieldNumber_SpeechContextsArray = 6,
  SPBRecognitionConfig_FieldNumber_EnableWordTimeOffsets = 8,
  SPBRecognitionConfig_FieldNumber_DisableAutomaticPunctuation = 11,
  SPBRecognitionConfig_FieldNumber_Model = 13,
  SPBRecognitionConfig_FieldNumber_ExtraConfigsArray = 20,
};

/**
 * Provides information to the recognizer that specifies how to process the
 * request.
 **/
@interface SPBRecognitionConfig : GPBMessage

/** *Required* Encoding of audio data sent in all `RecognitionAudio` messages. */
@property(nonatomic, readwrite) SPBRecognitionConfig_AudioEncoding encoding;

/**
 * *Required* Sample rate in Hertz of the audio data sent in all
 * `RecognitionAudio` messages. Valid values are only: 16000.
 **/
@property(nonatomic, readwrite) int32_t sampleRateHertz;

/**
 * *Required* The language of the supplied audio as a
 * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 * Example: "zh-cmn-Hans-CN".
 * See [Language Support] for a list of the currently supported language codes.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *languageCode;

/**
 * *Optional* Maximum number of recognition hypotheses to be returned.
 * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 * within each `SpeechRecognitionResult`.
 * The server may return fewer than `max_alternatives`.
 * Valid values are `0`-`5`. A value of `0` or `1` will return 1.
 * If omitted, will return 1.
 **/
@property(nonatomic, readwrite) int32_t maxAlternatives;

/**
 * *Optional* If set to `true`, the server will attempt to filter out
 * profanities
 **/
@property(nonatomic, readwrite) BOOL profanityFilter;

/** *Optional* A means to provide context to assist the speech recognition. */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBSpeechContext*> *speechContextsArray;
/** The number of items in @c speechContextsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger speechContextsArray_Count;

/**
 * *Optional* If `true`, the top result includes a list of words and
 * the start and end time offsets (timestamps) for those words. If
 * `false`, no word-level time offset information is returned. The default is
 * `false`.
 **/
@property(nonatomic, readwrite) BOOL enableWordTimeOffsets;

/**
 * *Optional* If 'true', stop adds punctuation to recognition result hypotheses.
 * The default 'false' value DOES add punctuation to result hypotheses.
 **/
@property(nonatomic, readwrite) BOOL disableAutomaticPunctuation;

/**
 * *Optional* Which model to select for the given request. Select the model
 * best suited to your domain to get best results. If a model is not
 * explicitly specified, "default" model is used.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *model;

/** *Optional* Configuration for some specific scenarios */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<GPBAny*> *extraConfigsArray;
/** The number of items in @c extraConfigsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger extraConfigsArray_Count;

@end

/**
 * Fetches the raw value of a @c SPBRecognitionConfig's @c encoding property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t SPBRecognitionConfig_Encoding_RawValue(SPBRecognitionConfig *message);
/**
 * Sets the raw value of an @c SPBRecognitionConfig's @c encoding property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetSPBRecognitionConfig_Encoding_RawValue(SPBRecognitionConfig *message, int32_t value);

#pragma mark - SPBSpeechContext

typedef GPB_ENUM(SPBSpeechContext_FieldNumber) {
  SPBSpeechContext_FieldNumber_PhrasesArray = 1,
};

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 **/
@interface SPBSpeechContext : GPBMessage

/**
 * *Optional* A list of strings containing words and phrases "hints" so that
 * the speech recognition is more likely to recognize them. This can be used
 * to improve the accuracy for specific words and phrases, for example, if
 * specific commands are typically spoken by the user. This can also be used
 * to add additional words to the vocabulary of the recognizer. See [usage limits]
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<NSString*> *phrasesArray;
/** The number of items in @c phrasesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger phrasesArray_Count;

@end

#pragma mark - SPBRecognitionAudio

typedef GPB_ENUM(SPBRecognitionAudio_FieldNumber) {
  SPBRecognitionAudio_FieldNumber_Content = 1,
  SPBRecognitionAudio_FieldNumber_Uri = 2,
};

typedef GPB_ENUM(SPBRecognitionAudio_AudioSource_OneOfCase) {
  SPBRecognitionAudio_AudioSource_OneOfCase_GPBUnsetOneOfCase = 0,
  SPBRecognitionAudio_AudioSource_OneOfCase_Content = 1,
  SPBRecognitionAudio_AudioSource_OneOfCase_Uri = 2,
};

/**
 * Contains audio data in the encoding specified in the `RecognitionConfig`.
 * Either `content` or `uri` must be supplied. Supplying both or neither
 * returns [google.rpc.Code.INVALID_ARGUMENT].
 **/
@interface SPBRecognitionAudio : GPBMessage

/** The audio source, which is either inline content or a uri. */
@property(nonatomic, readonly) SPBRecognitionAudio_AudioSource_OneOfCase audioSourceOneOfCase;

/**
 * The audio data bytes encoded as specified in
 * `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 * pure binary representation, whereas JSON representations use base64.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSData *content;

/**
 * ** CURRENTLY SOGOU INTERNAL USE ONLY. **
 * URI that points to a file that contains audio data bytes as specified in
 * `RecognitionConfig`. Currently, only SOGOU INTERNAL URIs are
 * supported, (other URI formats return [google.rpc.Code.INVALID_ARGUMENT]).
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *uri;

@end

/**
 * Clears whatever value was set for the oneof 'audioSource'.
 **/
void SPBRecognitionAudio_ClearAudioSourceOneOfCase(SPBRecognitionAudio *message);

#pragma mark - SPBRecognizeResponse

typedef GPB_ENUM(SPBRecognizeResponse_FieldNumber) {
  SPBRecognizeResponse_FieldNumber_ResultsArray = 1,
};

/**
 * The only message returned to the client by the `Recognize` method. It
 * contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages.
 **/
@interface SPBRecognizeResponse : GPBMessage

/**
 * Output only. Sequential list of transcription results corresponding to
 * sequential portions of audio.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBSpeechRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

@end

#pragma mark - SPBLongRunningRecognizeResponse

typedef GPB_ENUM(SPBLongRunningRecognizeResponse_FieldNumber) {
  SPBLongRunningRecognizeResponse_FieldNumber_ResultsArray = 1,
};

/**
 * The only message returned to the client by the `LongRunningRecognize` method.
 * It contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages. It is included in the `result.response` field of the `Operation`
 * returned by the `GetOperation` call of the `sogou.speech.longrunning.Operations`
 * service.
 **/
@interface SPBLongRunningRecognizeResponse : GPBMessage

@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBSpeechRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

@end

#pragma mark - SPBLongRunningRecognizeMetadata

typedef GPB_ENUM(SPBLongRunningRecognizeMetadata_FieldNumber) {
  SPBLongRunningRecognizeMetadata_FieldNumber_ProgressPercent = 1,
  SPBLongRunningRecognizeMetadata_FieldNumber_StartTime = 2,
  SPBLongRunningRecognizeMetadata_FieldNumber_LastUpdateTime = 3,
};

/**
 * Describes the progress of a long-running `LongRunningRecognize` call. It is
 * included in the `metadata` field of the `Operation` returned by the
 * `GetOperation` call of the `sogou.speech.longrunning.Operations` service.
 **/
@interface SPBLongRunningRecognizeMetadata : GPBMessage

/**
 * Approximate percentage of audio processed thus far. Guaranteed to be 100
 * when the audio is fully processed and the results are available.
 **/
@property(nonatomic, readwrite) int32_t progressPercent;

/** Time when the request was received. */
@property(nonatomic, readwrite, strong, null_resettable) GPBTimestamp *startTime;
/** Test to see if @c startTime has been set. */
@property(nonatomic, readwrite) BOOL hasStartTime;

/** Time of the most recent processing update. */
@property(nonatomic, readwrite, strong, null_resettable) GPBTimestamp *lastUpdateTime;
/** Test to see if @c lastUpdateTime has been set. */
@property(nonatomic, readwrite) BOOL hasLastUpdateTime;

@end

#pragma mark - SPBStreamingRecognizeResponse

typedef GPB_ENUM(SPBStreamingRecognizeResponse_FieldNumber) {
  SPBStreamingRecognizeResponse_FieldNumber_Error = 1,
  SPBStreamingRecognizeResponse_FieldNumber_ResultsArray = 2,
  SPBStreamingRecognizeResponse_FieldNumber_SpeechEventType = 4,
};

/**
 * `StreamingRecognizeResponse` is the only message returned to the client by
 * `StreamingRecognize`. A series of zero or more `StreamingRecognizeResponse`
 * messages are streamed back to the client. If there is no recognizable
 * audio, and `single_utterance` is set to false, then no messages are streamed
 * back to the client.
 * In each response, only one of these fields will be set:
 *   `error`,
 *   `speech_event_type`, or
 *   one or more (repeated) `results`.
 **/
@interface SPBStreamingRecognizeResponse : GPBMessage

/**
 * *Output-only* If set, returns a [google.rpc.Status] message that
 * specifies the error for the operation.
 **/
@property(nonatomic, readwrite, strong, null_resettable) RPCStatus *error;
/** Test to see if @c error has been set. */
@property(nonatomic, readwrite) BOOL hasError;

/**
 * *Output-only* This repeated list contains zero or more results that
 * correspond to consecutive portions of the audio currently being processed.
 * It contains zero or more `is_final=false` results followed by zero or one
 * `is_final=true` result (the newly settled portion).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBStreamingRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

/** *Output-only* Indicates the type of speech event. */
@property(nonatomic, readwrite) SPBStreamingRecognizeResponse_SpeechEventType speechEventType;

@end

/**
 * Fetches the raw value of a @c SPBStreamingRecognizeResponse's @c speechEventType property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t SPBStreamingRecognizeResponse_SpeechEventType_RawValue(SPBStreamingRecognizeResponse *message);
/**
 * Sets the raw value of an @c SPBStreamingRecognizeResponse's @c speechEventType property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetSPBStreamingRecognizeResponse_SpeechEventType_RawValue(SPBStreamingRecognizeResponse *message, int32_t value);

#pragma mark - SPBStreamingRecognitionResult

typedef GPB_ENUM(SPBStreamingRecognitionResult_FieldNumber) {
  SPBStreamingRecognitionResult_FieldNumber_AlternativesArray = 1,
  SPBStreamingRecognitionResult_FieldNumber_IsFinal = 2,
  SPBStreamingRecognitionResult_FieldNumber_Stability = 3,
  SPBStreamingRecognitionResult_FieldNumber_Extra = 4,
};

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 **/
@interface SPBStreamingRecognitionResult : GPBMessage

/**
 * *Output-only* May contain one or more recognition hypotheses (up to the
 * maximum specified in `max_alternatives`).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBSpeechRecognitionAlternative*> *alternativesArray;
/** The number of items in @c alternativesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger alternativesArray_Count;

/**
 * *Output-only* If `false`, this `StreamingRecognitionResult` represents an
 * interim result that may change. If `true`, this is the final time the
 * speech service will return this particular `StreamingRecognitionResult`,
 * the recognizer will not return any further hypotheses for this portion of
 * the transcript and corresponding audio.
 **/
@property(nonatomic, readwrite) BOOL isFinal;

/**
 * *Output-only* An estimate of the likelihood that the recognizer will not
 * change its guess about this interim result. Values range from 0.0
 * (completely unstable) to 1.0 (completely stable).
 * This field is only provided for interim results (`is_final=false`).
 * The default of 0.0 is a sentinel value indicating `stability` was not set.
 **/
@property(nonatomic, readwrite) float stability;

/** *Output-only* Other informations related to this recongition result */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableDictionary<NSString*, NSString*> *extra;
/** The number of items in @c extra without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger extra_Count;

@end

#pragma mark - SPBSpeechRecognitionResult

typedef GPB_ENUM(SPBSpeechRecognitionResult_FieldNumber) {
  SPBSpeechRecognitionResult_FieldNumber_AlternativesArray = 1,
  SPBSpeechRecognitionResult_FieldNumber_Extra = 2,
};

/**
 * A speech recognition result corresponding to a portion of the audio.
 **/
@interface SPBSpeechRecognitionResult : GPBMessage

/**
 * Output only. May contain one or more recognition hypotheses (up to the
 * maximum specified in `max_alternatives`).
 * These alternatives are ordered in terms of accuracy, with the top (first)
 * alternative being the most probable, as ranked by the recognizer.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBSpeechRecognitionAlternative*> *alternativesArray;
/** The number of items in @c alternativesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger alternativesArray_Count;

/** *Output-only* Other informations related to this recongition result */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableDictionary<NSString*, NSString*> *extra;
/** The number of items in @c extra without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger extra_Count;

@end

#pragma mark - SPBSpeechRecognitionAlternative

typedef GPB_ENUM(SPBSpeechRecognitionAlternative_FieldNumber) {
  SPBSpeechRecognitionAlternative_FieldNumber_Transcript = 1,
  SPBSpeechRecognitionAlternative_FieldNumber_Confidence = 2,
  SPBSpeechRecognitionAlternative_FieldNumber_WordsArray = 3,
  SPBSpeechRecognitionAlternative_FieldNumber_Extra = 4,
};

/**
 * Alternative hypotheses (a.k.a. n-best list).
 **/
@interface SPBSpeechRecognitionAlternative : GPBMessage

/** *Output-only* Transcript text representing the words that the user spoke. */
@property(nonatomic, readwrite, copy, null_resettable) NSString *transcript;

/**
 * *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
 * indicates an estimated greater likelihood that the recognized words are
 * correct. This field is typically provided only for the top hypothesis, and
 * only for `is_final=true` results. Clients should not rely on the
 * `confidence` field as it is not guaranteed to be accurate or consistent.
 * The default of 0.0 is a sentinel value indicating `confidence` was not set.
 **/
@property(nonatomic, readwrite) float confidence;

/** *Output-only* A list of word-specific information for each recognized word. */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SPBWordInfo*> *wordsArray;
/** The number of items in @c wordsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger wordsArray_Count;

/** *Output-only* Other informations related to this transcript text */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableDictionary<NSString*, NSString*> *extra;
/** The number of items in @c extra without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger extra_Count;

@end

#pragma mark - SPBWordInfo

typedef GPB_ENUM(SPBWordInfo_FieldNumber) {
  SPBWordInfo_FieldNumber_StartTime = 1,
  SPBWordInfo_FieldNumber_EndTime = 2,
  SPBWordInfo_FieldNumber_Word = 3,
};

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 **/
@interface SPBWordInfo : GPBMessage

/**
 * *Output-only* Time offset relative to the beginning of the audio,
 * and corresponding to the start of the spoken word.
 * This field is only set if `enable_word_time_offsets=true` and only
 * in the top hypothesis.
 * This is an experimental feature and the accuracy of the time offset can
 * vary.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBDuration *startTime;
/** Test to see if @c startTime has been set. */
@property(nonatomic, readwrite) BOOL hasStartTime;

/**
 * *Output-only* Time offset relative to the beginning of the audio,
 * and corresponding to the end of the spoken word.
 * This field is only set if `enable_word_time_offsets=true` and only
 * in the top hypothesis.
 * This is an experimental feature and the accuracy of the time offset can
 * vary.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBDuration *endTime;
/** Test to see if @c endTime has been set. */
@property(nonatomic, readwrite) BOOL hasEndTime;

/** *Output-only* The word corresponding to this set of information. */
@property(nonatomic, readwrite, copy, null_resettable) NSString *word;

@end

NS_ASSUME_NONNULL_END

CF_EXTERN_C_END

#pragma clang diagnostic pop

// @@protoc_insertion_point(global_scope)
